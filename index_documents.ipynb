{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('test.txt')\n",
    "lines=f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=[]\n",
    "for l in lines:\n",
    "    documents.append(l[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_terms=set()\n",
    "num_terms=0\n",
    "avg_dl=0\n",
    "for d in documents:\n",
    "    terms=d.split()\n",
    "    unique_terms |= set(terms)\n",
    "    num_terms+=len(terms)\n",
    "avg_dl=num_terms/num_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_terms=len(unique_terms)\n",
    "total_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.2"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_in_docs={}\n",
    "corpus_term_count={}\n",
    "doc_term_count={}\n",
    "doc_size={}\n",
    "doc_unique_term_nums={}\n",
    "doc_unique_terms={}\n",
    "est_tf={}\n",
    "btf={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in unique_terms:\n",
    "    term_in_docs[each]=[]\n",
    "    doc_term_count[each]={}\n",
    "    est_tf[each]={}\n",
    "    btf[each]={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,d in enumerate(documents):\n",
    "    terms=d.split()\n",
    "    for each in unique_terms:\n",
    "        if each in terms:\n",
    "            term_in_docs[each].append(i)\n",
    "    for t in terms:\n",
    "        if t not in corpus_term_count:\n",
    "            corpus_term_count[t]=1\n",
    "        else:\n",
    "            corpus_term_count[t]+=1\n",
    "        if i not in doc_term_count[t]:          \n",
    "            doc_term_count[t][i]=1\n",
    "        else:\n",
    "            doc_term_count[t][i]+=1\n",
    "    doc_unique_term_nums[i]=len(set(terms))\n",
    "    doc_unique_terms[i]=set(terms)\n",
    "    doc_size[i]=len(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_count={}\n",
    "for each in term_in_docs:\n",
    "    doc_count[each]=len(term_in_docs[each])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in est_tf:\n",
    "    for i,d in enumerate(documents):\n",
    "        freq=0\n",
    "        for dut in doc_unique_terms[i]:\n",
    "            freq+=corpus_term_count[dut]\n",
    "        est_tf[t][i]=1+(corpus_term_count[t]/freq)*(doc_size[i]-doc_unique_term_nums[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd={}\n",
    "for i,d in enumerate(documents):\n",
    "        freq=0\n",
    "        for dut in doc_unique_terms[i]:\n",
    "            freq+=(doc_term_count[dut][i]/est_tf[dut][i])\n",
    "        cd[i]=freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in est_tf:\n",
    "    for i,d in enumerate(documents):\n",
    "        if i not in doc_term_count[t]:\n",
    "            doc_term_count_i=0\n",
    "        else:\n",
    "            doc_term_count_i=doc_term_count[t][i]\n",
    "        btf[t][i]=cd[i]*(doc_term_count_i/est_tf[t][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta={}\n",
    "meta['avg_dl']=avg_dl\n",
    "meta['num_docs']=num_docs\n",
    "meta['total_terms']=total_terms\n",
    "meta['doc_count']=doc_count\n",
    "meta['corpus_term_count']=corpus_term_count\n",
    "meta['doc_term_count']=doc_term_count\n",
    "meta['doc_size']=doc_size\n",
    "meta['doc_unique_term_nums']=doc_unique_term_nums\n",
    "meta['doc_unique_terms']=doc_unique_terms\n",
    "meta['btf']=btf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def savePickle(dictfile,filename):\n",
    "\tprint(\"saving....\")\n",
    "\tf=open(filename+\".pickle\",'wb')\n",
    "\tpickle.dump(dictfile,f)\n",
    "\tf.close()\n",
    "\n",
    "def loadPickle(filename):\n",
    "\tprint(\"loading....\")\n",
    "\tf=open(filename+\".pickle\",'rb')\n",
    "\tdictfile=pickle.load(f)\n",
    "\tf.close()\n",
    "\treturn dictfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving....\n"
     ]
    }
   ],
   "source": [
    "savePickle(meta,'meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
